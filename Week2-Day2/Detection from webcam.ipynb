{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('./assets/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('./assets/haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex, ey ,ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 0, 255), 5)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the classifier for frontal face and eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('./assets/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('./assets/haarcascade_eye_tree_eyeglasses.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cv2.VideoCapture() to get a video capture object for the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We can detect faces in videos.Videos are basically made up of frames, which are still images. So we perform the face detection for each frame in a video. \n",
    "\n",
    "#### We use an infinite loop to loop through each frame in the video. We use cap.read() to read each frame. The first value returned is a flag that indicates if the frame was read correctly or not. We don’t need it. The second value returned is the still frame on which we will be performing the detection. \n",
    "\n",
    "####  Now for each frame, we have to classify the face so it is inside while loop. We first read the frame, then convert to a grayscale of each frame and then detects faces of different sizes in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we're finding faces, their sizes, drawing rectangles, and noting the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting eyes and making rectangles around eyes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "for (ex, ey ,ew, eh) in eyes:\n",
    "    cv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 0, 255), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('img', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cv2.waitKey() is a required building block for OpenCV video processing. waitKey is a method which displays the frame for specified milliseconds. The ‘0xFF == ord(‘q’)’ inside the ‘if’ statement is a special syntax to provide the ‘while’ loop break, by a keyboard key pressing event.\n",
    "#### After wait of 1ms if the user press q on the keyboard, it will come out of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cap.release() is a method to close video files or the capturing device which was created by the imshow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
